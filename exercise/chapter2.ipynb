{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - The four fundamental equations behind backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate presentation of the equations of backpropagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose there are $n$ neurons in the output layer. So the square maxtrix, $\\sum^{'}(z^L)$, can be represented by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "        \\begin{bmatrix}\n",
    "        \\sigma'(z^L_1) & 0 & 0 & \\cdots & 0 \\\\\n",
    "        0 & \\sigma'(z^L_2) & 0 & \\cdots & 0 \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        0 & 0 & 0 & \\cdots & \\sigma'(z^L_n) \\\\\n",
    "        \\end{bmatrix} ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\triangledown_aC$ is a nx1-dimensional matrix, which can be represented by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "        \\begin{bmatrix}\n",
    "        \\frac{\\partial C}{\\partial a^L_1} \\\\\n",
    "        \\frac{\\partial C}{\\partial a^L_2} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\frac{\\partial C}{\\partial a^L_n}  \\\\\n",
    "        \\end{bmatrix} .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by multipling these two matrices we can get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "        \\begin{bmatrix}\n",
    "        \\sigma'(z^L_1) \\cdot \\frac{\\partial C}{\\partial a^L_1} \\\\\n",
    "        \\sigma'(z^L_2) \\cdot \\frac{\\partial C}{\\partial a^L_2} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\sigma'(z^L_n) \\cdot \\frac{\\partial C}{\\partial a^L_n}  \\\\\n",
    "        \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easily to vetify that Equation (BP1) gives the same result as this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $\\triangledown_a'C = ((w^{l+1})^T\\delta^{l+1})$, then Equation (BP2) can be rewritten as\n",
    "$$\\delta^L = \\triangledown_a'C\\odot\\sigma'(z^L) ,$$\n",
    "\n",
    "and Equation 2.12 can be rewritten as\n",
    "$$\\delta^L = \\sum'(z^L)\\triangledown_a'C .$$\n",
    "\n",
    "You will find that we have demonstrated this case in (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\delta^l = \\sum'(z^l)(w^{l+1})^T\\delta^{l+1}$, and $\\delta^{l+1} = \\sum'(z^{l+1})(w^{l+2})^T\\delta^{l+2}$, and so on. We can easily get the result \n",
    "\n",
    "$$\\delta^l = \\sum'(z^l)(w^{l+1})^T...\\sum'(z^{L-1})(w^{L})^T\\delta^{L} = \\sum'(z^l)(w^{l+1})^T...\\sum'(z^{L-1})(w^{L})^T  \\sum'(z^L)\\triangledown_aC $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 - Proof of the four fundamental equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with (BP3), which relates $\\delta^l_j$ to $\\partial C / \\partial b^l_j$. We can use the chain rule\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^l_j} = \\sum_{k}\\frac{\\partial C}{\\partial z^l_k}\\frac{\\partial z^l_k}{\\partial b^l_j}, \\tag{2.16}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where sum is over all neurons $k$ in the $l$-th layer. Of course, the weighted input $z^l_k$ of the $k$-th neuron depends only on the bias $b^l_j$ of the $j$-th neuron when $k=j$. And so $\\partial z^l_k / \\partial b^l_j$ vanishes when $k \\ne j$. As a result we can simplify the previous equation to\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z^l_j} \\frac{\\partial z^l_j}{\\partial b^l_j}. \\tag{2.17}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling that $\\delta^l_j = \\partial C / \\partial z^l_j$ the first term on the left can be rewritten as $\\delta^l_j$. To evaluate the second term on the last line, note that\n",
    "\n",
    "$$z^l_j = \\sum_{k}w^{l}_{jk}a^{l-1}_k + b^l_j. \\tag{2.18}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating, we obtain\n",
    "\n",
    "$$\\frac{\\partial z^l_j}{\\partial b^l_j} = 1. \\tag{2.19}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting back into (2.17) we obtain\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^l_j} = \\delta ^l_j, \\tag{2.20}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is just (BP3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll prove (BP4), which relates $\\delta^l_j$ to $\\partial C / \\partial w^l_{jk}$. We can use the chain rule\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}} = \\sum_{m}\\frac{\\partial C}{\\partial z^l_m}\\frac{\\partial z^l_m}{\\partial w^l_{jk}}, \\tag{2.21}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where sum is over all neurons $m$ in the $l$-th layer. Also, the weighted input $z^l_m$ of the $m$-th neuron only depends on the weight $w^l_{jk}$ when $m=j$. And so $\\partial z^l_m / \\partial w^l_{jk}$ vanishes when $m \\ne j$. As a result we can simplify the previous equation to\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial w^l_{jk}}. \\tag{2.22}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling that $\\delta^l_j = \\partial C / \\partial z^l_j$ the first term on the left can be rewritten as $\\delta^l_j$. To evaluate the second term on the right, note that\n",
    "\n",
    "$$z^l_j = \\sum_{m}w^l_{jm}a^{l-1}_m + b^l_j. \\tag{2.23}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating, we obtain\n",
    "\n",
    "$$\\frac{\\partial z^l_j}{\\partial w^l_{jk}} = a^{l-1}_k. \\tag{2.24}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting back into (2.24) we obtain\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}} = \\delta^l_j a^{l-1}_k. \\tag{2.25}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just (BP4). That completes the proof of the two fundamental equations of backpropagation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.4 - The four fundamental equations behind backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternate presentation of the equations of backpropagation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose there are $n$ neurons in the output layer. So the square maxtrix, $\\sum^{'}(z^L)$, can be represented by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "        \\begin{bmatrix}\n",
    "        \\sigma'(z^L_1) & 0 & 0 & \\cdots & 0 \\\\\n",
    "        0 & \\sigma'(z^L_2) & 0 & \\cdots & 0 \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "        0 & 0 & 0 & \\cdots & \\sigma'(z^L_n) \\\\\n",
    "        \\end{bmatrix} ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and $\\triangledown_aC$ is a nx1-dimensional matrix, which can be represented by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "        \\begin{bmatrix}\n",
    "        \\frac{\\partial C}{\\partial a^L_1} \\\\\n",
    "        \\frac{\\partial C}{\\partial a^L_2} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\frac{\\partial C}{\\partial a^L_n}  \\\\\n",
    "        \\end{bmatrix} .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by multipling these two matrices we can get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "        \\begin{bmatrix}\n",
    "        \\sigma'(z^L_1) \\cdot \\frac{\\partial C}{\\partial a^L_1} \\\\\n",
    "        \\sigma'(z^L_2) \\cdot \\frac{\\partial C}{\\partial a^L_2} \\\\\n",
    "        \\vdots \\\\\n",
    "        \\sigma'(z^L_n) \\cdot \\frac{\\partial C}{\\partial a^L_n}  \\\\\n",
    "        \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easily to vetify that Equation (BP1) gives the same result as this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose $\\triangledown_a'C = ((w^{l+1})^T\\delta^{l+1})$, then Equation (BP2) can be rewritten as\n",
    "$$\\delta^L = \\triangledown_a'C\\odot\\sigma'(z^L) ,$$\n",
    "\n",
    "and Equation 2.12 can be rewritten as\n",
    "$$\\delta^L = \\sum'(z^L)\\triangledown_a'C .$$\n",
    "\n",
    "You will find that we have demonstrated this case in (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\delta^l = \\sum'(z^l)(w^{l+1})^T\\delta^{l+1}$, and $\\delta^{l+1} = \\sum'(z^{l+1})(w^{l+2})^T\\delta^{l+2}$, and so on. We can easily get the result \n",
    "\n",
    "$$\\delta^l = \\sum'(z^l)(w^{l+1})^T...\\sum'(z^{L-1})(w^{L})^T\\delta^{L} = \\sum'(z^l)(w^{l+1})^T...\\sum'(z^{L-1})(w^{L})^T  \\sum'(z^L)\\triangledown_aC $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5 - Proof of the four fundamental equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin with (BP3), which relates $\\delta^l_j$ to $\\partial C / \\partial b^l_j$. We can use the chain rule\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^l_j} = \\sum_{k}\\frac{\\partial C}{\\partial z^l_k}\\frac{\\partial z^l_k}{\\partial b^l_j}, \\tag{2.16}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where sum is over all neurons $k$ in the $l$-th layer. Of course, the weighted input $z^l_k$ of the $k$-th neuron depends only on the bias $b^l_j$ of the $j$-th neuron when $k=j$. And so $\\partial z^l_k / \\partial b^l_j$ vanishes when $k \\ne j$. As a result we can simplify the previous equation to\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^l_j} = \\frac{\\partial C}{\\partial z^l_j} \\frac{\\partial z^l_j}{\\partial b^l_j}. \\tag{2.17}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling that $\\delta^l_j = \\partial C / \\partial z^l_j$ the first term on the left can be rewritten as $\\delta^l_j$. To evaluate the second term on the last line, note that\n",
    "\n",
    "$$z^l_j = \\sum_{k}w^{l}_{jk}a^{l-1}_k + b^l_j. \\tag{2.18}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating, we obtain\n",
    "\n",
    "$$\\frac{\\partial z^l_j}{\\partial b^l_j} = 1. \\tag{2.19}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting back into (2.17) we obtain\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^l_j} = \\delta ^l_j, \\tag{2.20}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is just (BP3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll prove (BP4), which relates $\\delta^l_j$ to $\\partial C / \\partial w^l_{jk}$. We can use the chain rule\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}} = \\sum_{m}\\frac{\\partial C}{\\partial z^l_m}\\frac{\\partial z^l_m}{\\partial w^l_{jk}}, \\tag{2.21}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where sum is over all neurons $m$ in the $l$-th layer. Also, the weighted input $z^l_m$ of the $m$-th neuron only depends on the weight $w^l_{jk}$ when $m=j$. And so $\\partial z^l_m / \\partial w^l_{jk}$ vanishes when $m \\ne j$. As a result we can simplify the previous equation to\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}} = \\frac{\\partial C}{\\partial z^l_j}\\frac{\\partial z^l_j}{\\partial w^l_{jk}}. \\tag{2.22}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalling that $\\delta^l_j = \\partial C / \\partial z^l_j$ the first term on the left can be rewritten as $\\delta^l_j$. To evaluate the second term on the right, note that\n",
    "\n",
    "$$z^l_j = \\sum_{m}w^l_{jm}a^{l-1}_m + b^l_j. \\tag{2.23}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating, we obtain\n",
    "\n",
    "$$\\frac{\\partial z^l_j}{\\partial w^l_{jk}} = a^{l-1}_k. \\tag{2.24}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting back into (2.24) we obtain\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^l_{jk}} = \\delta^l_j a^{l-1}_k. \\tag{2.25}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just (BP4). That completes the proof of the two fundamental equations of backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.6 - The backpropagation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation with a single modified neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, in the first step, we should compute the activation of this neuron by activation function, $f$, instead of $\\sigma$. Secondly, when compute the error of this neuron, we should use $f'$ instead of $\\sigma'$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation with linear neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, $\\sigma'(z^l_j)$ equals to 1, which is a constant. So, we just need to replace $a^l = \\sigma(z^l)$ and $\\sigma'(z^l)$ to $a^l = z^l$ and 1 respectively in the above algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.7 - The code for backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully matrix-based approach to backpropagation over a mini-batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It's possible to modify the backpropagation algorithm so that it computes the gradients for all training examples in a mini-batch simultaneously. The idea is that instead of beginning with a single input vector, $x$, we can begin with a matrix $X = [x_1x_2...x_m]$ whose columns are the vectors in the mini-batch. The advantage of this approach is that it takes full advantage of modern libraries for linear algebra. As a result it can be quite a bit faster than looping over the mini-batch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we should know a feature of most modern libraries for linear algebra, namely so-called $broadcast$, which used in our code for this approach to the backpropagation algorithm. You can find some information about the usage of broadcast of the library called $numpy$ [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.broadcast.html#numpy.broadcast)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following algorithm computes the gradients for all training examples in a mini-batch simultaneously:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Input $X$**: (Note that here the matrix $X = [x_1x_2...x_m]$ whose columns are the vectors in the mini-batch and $m$ is the mini-batch size.) Set the activation $a^1 = X$ for the input layer.\n",
    "2. **Feedforward**: For each $l = 2, 3,..., L$ compute $z^l = w^la^{l-1} + b^l$ and $a^l = \\sigma(z^l).$\n",
    "3. **Output error $\\delta^L$**: Compute the matrix $\\delta^L = \\triangledown_aC\\odot\\sigma'(z^L)$ (Note that here $\\triangledown_aC$ and $z^L$ are both nxm-dimensional matrices, where n is the number of neurons in the output layer.).\n",
    "4. **Backpropagate the error**: For each $l = L-1, L-2,..., 2$ compute $\\delta^l = ((w^{l+1})^T\\delta^{l+1})\\odot\\sigma'(z^l).$ \n",
    "5. **Output**: The gradient of the cost function is given by $\\frac{\\partial C}{\\partial w^l} = \\delta^l(a^{l-1})^T$ and $\\frac{\\partial C}{\\partial b^l} = \\sum\\delta^l$, where $\\sum$ denote that add the matrix $\\delta^l$ by column."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 - The cross-entropy cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1.1 - Introducing the cross-entropy cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Ⅰ\n",
    "Verify that $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is defined by\n",
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}. \\tag{3.5}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the derivative for z, $\\sigma'(z)$, using calculus, obtaining:\n",
    "\n",
    "$$\\sigma'(z) = \\frac{e^{-z}}{(1 + e^{-z})^2}. \\tag{3.6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By computing the right hand-side of the equation that we want to verify, we get\n",
    "\n",
    "$$\\sigma(z)(1-\\sigma(z)) = \\frac{1}{1+e^{-z}} \\frac{e^{-z}}{(1 + e^{-z})} = \\frac{e^{-z}}{(1 + e^{-z})^2}. \\tag{3.7}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the Equation (3.6) with the Equation (3.7), we can draw a conclusion that $\\sigma'(z) = \\sigma(z)(1-\\sigma(z))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Ⅱ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the second expression $y$ should satisfy that $0 < y < 1$. It means that both $y = 0$ or $y = 1$ are illegal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem doesn't afflict the first expression because of the sigmoid function. The range of the sigmoid function is (0, 1), not [0, 1]. It means $a$ will not equal to 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Ⅲ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can consider cross-entropy as a function about $a$, and it turns out that $a = y$ is the only minimum point of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Ⅰ - Many-layer multi-neuron networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining with (BP4) and (BP1), we obtain\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^L{jk}} = a^{L-1}_{k}\\frac{\\partial C}{\\partial a^L_j}\\sigma'(z^L_j). \\tag{3.8}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have $\\partial C / \\partial a^L_j = (a^L_j - y_j)$. Substituting back into (3.8) we obtain\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^L{jk}} = a^{L-1}_{k}(a^L_j - y_j)\\sigma'(z^L_j). \\tag{3.9}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the cross-entropy cost the output error $\\delta^L$ for a single training example $x$ is given by\n",
    "\n",
    "$$\\delta^L_j = \\frac{\\partial C}{\\partial a^L_j}\\sigma'(z^L_j). \\tag{3.10}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial a^L_j} = \\frac{a^L_j - y_j}{a^L_j(1-a^L_j)} = \\frac{a^L_j - y_j}{\\sigma(z^L_j)(1-\\sigma(z^L_j))}. \\tag{3.11}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\sigma'(z^L_j) = \\sigma(z^L_j)(1-\\sigma(z^L_j))$, so the Equation becomes\n",
    "\n",
    "$$\\delta^L_j = a^L_j - y_j. \\tag{3.12}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The partial derivative with respect to the weights in the output layer is given by\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^L_{jk}} = a^{L-1}_k\\delta^L_j = a^{L-1}_{k}(a^L_j - y_j). \\tag{3.13}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $\\sigma'(z^L_j)$ term has vanished, and so the cross-entropy avoids the problem of learning slowdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Ⅱ - Using the quadratic cost when we have linear neurons in the output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it's equivalent to let $\\sigma'(z^L_j)$ equal to 1. So the output error $\\delta^L$ for a single training example $x$ is given by\n",
    "\n",
    "$$\\delta^L_j = \\frac{\\partial C}{\\partial a^L_j}\\sigma'(z^L_j) = a^L-y. \\tag{3.14}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to (BP3) and (BP4), it's easy to get the result that\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial w^L_{jk}} = a^{L-1}_k\\delta^L_j = a^{L-1}_{k}(a^L_j - y_j). \\tag{3.15}$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^L_{j}} = \\delta^L_j = (a^L_j - y_j). \\tag{3.16}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This shows that if the output neurons are linear neurons then the quadratic cost will not give rise to any problems with a learning slowdown.(But how about other neurons in the network?)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

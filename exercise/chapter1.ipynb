{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-2 Sigmoid neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid neurons simulating perceptrons, part Ⅰ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of a single perceptron can be calculated in the following way:\n",
    "\n",
    "![perceptron](../picture/chapter1/p3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when we mutiply a positive constant, c, then the inequality that $w\\cdot x\\cdot c + b\\cdot c \\le 0$ still holds if $w\\cdot x + b \\le 0$. The another situation that $w\\cdot x + b > 0$ can also be demonstated in the same way easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we mutiply all the weights and biases in a network of perceptrons by a postive number, the behavior of the network doesn't change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid neurons simulating perceptrons, part Ⅱ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is defined by:\n",
    "\n",
    "$$\\sigma(z) \\equiv \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because of the weights and biases are such that $w\\cdot x + b \\ne 0$ for the input x to any particular perceptron in the network, so this situation can be divided into two cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $w \\cdot x + b > 0$. In this case, the output of a single perceptron is 1. And if we replace the perceptron by sigmoid neuron and multiply the weights and biases by a positive constant c > 0, the output of the sigmoid neuron is also 1, since $z = \\lim_{c \\to +\\infty} w \\cdot x \\cdot c + b \\cdot c = +\\infty$ and $\\sigma(z) = 1.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $w \\cdot x + b < 0$. In this case, the output of a single perceptron is 0. And we do the same like above, the output of the sigmoid neuron is also 0, since $z = \\lim_{c \\to +\\infty} w \\cdot x \\cdot c + b \\cdot c = -\\infty$ and $\\sigma(z) = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this fail when $w \\cdot x + b = 0$ for one of the perceptrons because the output of this perceptron is 0 which not equal to the output, 0.5, when we replace the perceptron by sigmoid neuron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-4 A simple network to classify handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-5 Learning with gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Ⅰ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Ⅱ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When C is a function of just one variable, e.g., $C = f(v)$. If we move the ball a small amount $\\Delta v_1$, calculus tells us that $C$ will change as follows:\n",
    "\n",
    "$$\\Delta C \\approx \\frac{df(v)}{dv}\\Delta v\\tag{1.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can choose $\\Delta v = -\\eta\\frac{df(v)}{dv}$, where $\\eta$ is a small, positive parameter. Equation 1.1 tells us that $\\Delta C = -\\eta\\|\\frac{df(v)}{dv}\\|^2$, so $C$ will always decrease　just as the situation that $C$ is a function of more than one variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A geometric interpretation is shown as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![exercise](../picture/chapter1/p10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Ⅲ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of online learning is that we can speed up gradient descent, and thus learning.\n",
    "\n",
    "One disadvantage of online learning is that there is no guarantee that the gradient of only one sample can estimate the gradient $\\triangledown C$ for the entire set of training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-6 Implementing our network to classify digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Ⅰ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we use the following three-layer neural network to recognize individual digits\n",
    "\n",
    "![network](../picture/chapter1/p9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the biases and weights are stored as lists of Numpy matrices. So, for example the Numpy matrix whose shape is (15, 784) stored the weights connecting the first and second layers of neurons, let's just denote that matrix $w$. **It's a matrix such that $w_{jk}$ is the weight for the connection between the $k$-th neuron in the first layer, and the $j$-th neuron in the third layer.** Also the Numpy matrix whose shape is (15, 1) stored the biases of neurons in the second layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the outputs of neurons in the second layer, let's input one image of 784 pixels, using a Numpy matrix , $a$, whose shape is (784, 1) to represent it. Then we do linear algebra $wa + b$ and the result is a Numpy matrix, $z$, whose shape is (15, 1). We then apply the function $\\sigma$ elementwise to every entry in the matrix $a'=\\sigma(z)$ and get outputs of neurons in the second layer. The calculation process is shown blew:\n",
    "![code](../picture/chapter1/p11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Ⅱ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a partial transcript of the output of two-layers network I trained:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "Epoch 0 : 6053 / 10000\n",
    "Epoch 1 : 6475 / 10000\n",
    "Epoch 2 : 7416 / 10000\n",
    "...\n",
    "Epoch 27 : 8382 / 10000\n",
    "Epoch 28 : 8394 / 10000\n",
    "Epoch 29 : 8374 / 10000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the performance of this two-layers network is much less encouraging than the three-layers network. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-2 Sigmoid neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid neurons simulating perceptrons, part Ⅰ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of a single perceptron can be calculated in the following way:\n",
    "\n",
    "![perceptron](../picture/chapter1/p3.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And when we mutiply a positive constant, c, then the inequality that $w\\cdot x\\cdot c + b\\cdot c \\le 0$ still holds if $w\\cdot x + b \\le 0$. The another situation that $w\\cdot x + b > 0$ can also be demonstated in the same way easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when we mutiply all the weights and biases in a network of perceptrons by a postive number, the behavior of the network doesn't change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid neurons simulating perceptrons, part Ⅱ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sigmoid function is defined by:\n",
    "\n",
    "$$\\sigma(z) \\equiv \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And because of the weights and biases are such that $w\\cdot x + b \\ne 0$ for the input x to any particular perceptron in the network, so this situation can be divided into two cases:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $w \\cdot x + b > 0$. In this case, the output of a single perceptron is 1. And if we replace the perceptron by sigmoid neuron and multiply the weights and biases by a positive constant c > 0, the output of the sigmoid neuron is also 1, since $z = \\lim_{c \\to +\\infty} w \\cdot x \\cdot c + b \\cdot c = +\\infty$ and $\\sigma(z) = 1.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $w \\cdot x + b < 0$. In this case, the output of a single perceptron is 0. And we do the same like above, the output of the sigmoid neuron is also 0, since $z = \\lim_{c \\to +\\infty} w \\cdot x \\cdot c + b \\cdot c = -\\infty$ and $\\sigma(z) = 0.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this fail when $w \\cdot x + b = 0$ for one of the perceptrons because the output of this perceptron is 0 which not equal to the output, 0.5, when we replace the perceptron by sigmoid neuron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-4 A simple network to classify handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-5 Learning with gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Ⅰ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Ⅱ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When C is a function of just one variable, e.g., $C = f(v)$. If we move the ball a small amount $\\Delta v_1$, calculus tells us that $C$ will change as follows:\n",
    "\n",
    "$$\\Delta C \\approx \\frac{df(v)}{dv}\\Delta v\\tag{1.1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can choose $\\Delta v = -\\eta\\frac{df(v)}{dv}$, where $\\eta$ is a small, positive parameter. Equation 1.1 tells us that $\\Delta C = -\\eta\\|\\frac{df(v)}{dv}\\|^2$, so $C$ will always decrease　just as the situation that $C$ is a function of more than one variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A geometric interpretation is shown as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![exercise](../picture/chapter1/p10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Ⅲ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One advantage of online learning is that we can speed up gradient descent, and thus learning.\n",
    "\n",
    "One disadvantage of online learning is that there is no guarantee that the gradient of only one sample can estimate the gradient $\\triangledown C$ for the entire set of training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

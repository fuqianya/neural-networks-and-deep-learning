{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What this book is about?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book introduces one of the most beautiful programming paradigms so-called neural networks. In the conventional approach to programming, **we tell the computer what to do**. By contrast, in a neural network we don't tell the computer how to solve our problem. Instead, **it learns from observational data**, **figuring out its own solution to the problem**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this book is to **help you master the core concepts of neural networks**. One conviction underlying the book is that **it's better to obtain a solid understanding of the core principles of neural networks and deep learning**, rather than a hazy understanding of a long laundry list of ideas. If you have understood the core ideas well, you can rapidly understand other new material. **You need to understand the durable, lasting insights underlying how neural networks work.**\n",
    "\n",
    "**Technologies come and go, but insight is forever.**\n",
    "\n",
    "You will learn the core principles behind neural networks and deep learning by attacking a concrete problem: the problem of teaching a computer to recongnize handwritten digits. This book will not only include abstract theory but also living code, and you will understand the fundamentals **both in theory and practice.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Using nerual nets to recognize handwritten digits\n",
    "## 1.1 - Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are a type of artificial neuron. Though today it is more common to use other models of artificial neurons on neural networks, and the main neuron model used is one called $sigmoid\\;neuron$. But to understand why sigmoid neurons are defined the way they are, it is worth taking time to first understand perceptrons.\n",
    "\n",
    "A perceptron takes several binary inputs,$x_1$,$x_2$,..., and produces a single binary output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron](../picture/chapter1/p1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each input with a corresponding $weight$ to compute the output. The weights are real numbers represent the importance of the respective inputs to the output. The larger value of $w_1$ indicates that the input $x_1$ is more important. The neuron's output, 0 or 1, is determined by whether the weighted sum $\\sum_j w_jx_j$ is less than or greater than some $threshold\\;value$. Just like the weights, the threshold is a real number which is a parameter of the neuron. To put it in more precise algebraic terms:\n",
    "![perceptron](../picture/chapter1/p2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make two notational changes to simplify this description. The first change if to write $\\sum_j w_jx_j$ as a dot product, $w\\cdot x = \\sum_j w_jx_j$, where $w$ and $x$ are **vectors** whose components are the weights and inputs respectively. The second change is to remove the threshold to the other side of the inequality and to replace it by $bias$, $b \\equiv -threshold$. So bias can be consider as a measure of how easy it is to get the perceptron to output a 1. Then the preceptron rule can be rewritten:\n",
    "![perceptron](../picture/chapter1/p3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use perceptrons to both make decisions and compute the elementary logical functions such as AND, OR, and NAND. \n",
    "\n",
    "Let $x_1$, $x_2$,..., represent the evidence of a decision respectively and $w_1$, $w_2$,..., indicate the importance of corresponding evidence. Finally, you should only define the $bias$ and the value of the $bias$ means how desirable you want to make that decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following perceptron has two inputs, each with weight -2, and an overall bias of 3 like:\n",
    "![perceptron](../picture/chapter1/p4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we see that input 00 produces output 1, since (-2)x0 + (-2)x0 + 3 = 3 is positive. Similar calculations show that the inputs 01 and 10 produce output 1. But the input 11 produces output 0, since (-2)x1 + (-2)x1 + 3 = -1 is negative. And so our perceptron implements a NAND gate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becasue the NAND gate is universal for computation, that is, we can build any computation up out of NAND gate, so we can use networks of perceptrons to compute any logical function at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

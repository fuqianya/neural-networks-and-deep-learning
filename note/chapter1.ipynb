{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What this book is about?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This book introduces one of the most beautiful programming paradigms so-called neural networks. In the conventional approach to programming, **we tell the computer what to do**. By contrast, in a neural network we don't tell the computer how to solve our problem. Instead, **it learns from observational data**, **figuring out its own solution to the problem**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this book is to **help you master the core concepts of neural networks**. One conviction underlying the book is that **it's better to obtain a solid understanding of the core principles of neural networks and deep learning**, rather than a hazy understanding of a long laundry list of ideas. If you have understood the core ideas well, you can rapidly understand other new material. **You need to understand the durable, lasting insights underlying how neural networks work.**\n",
    "\n",
    "**Technologies come and go, but insight is forever.**\n",
    "\n",
    "You will learn the core principles behind neural networks and deep learning by attacking a concrete problem: the problem of teaching a computer to recongnize handwritten digits. This book will not only include abstract theory but also living code, and you will understand the fundamentals **both in theory and practice.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Using nerual nets to recognize handwritten digits\n",
    "## 1.1 - Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptrons are a type of artificial neuron. Though today it is more common to use other models of artificial neurons on neural networks, and the main neuron model used is one called $sigmoid\\;neuron$. But to understand why sigmoid neurons are defined the way they are, it is worth taking time to first understand perceptrons.\n",
    "\n",
    "A perceptron takes several binary inputs,$x_1$,$x_2$,..., and produces a single binary output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![perceptron](../picture/chapter1/p1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each input with a corresponding $weight$ to compute the output. The weights are real numbers represent the importance of the respective inputs to the output. The larger value of $w_1$ indicates that the input $x_1$ is more important. The neuron's output, 0 or 1, is determined by whether the weighted sum $\\sum_j w_jx_j$ is less than or greater than some $threshold\\;value$. Just like the weights, the threshold is a real number which is a parameter of the neuron. To put it in more precise algebraic terms:\n",
    "![perceptron](../picture/chapter1/p2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make two notational changes to simplify this description. The first change if to write $\\sum_j w_jx_j$ as a dot product, $w\\cdot x = \\sum_j w_jx_j$, where $w$ and $x$ are **vectors** whose components are the weights and inputs respectively. The second change is to remove the threshold to the other side of the inequality and to replace it by $bias$, $b \\equiv -threshold$. So bias can be consider as a measure of how easy it is to get the perceptron to output a 1. Then the preceptron rule can be rewritten:\n",
    "![perceptron](../picture/chapter1/p3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use perceptrons to both make decisions and compute the elementary logical functions such as AND, OR, and NAND. \n",
    "\n",
    "Let $x_1$, $x_2$,..., represent the evidence of a decision respectively and $w_1$, $w_2$,..., indicate the importance of corresponding evidence. Finally, you should only define the $bias$ and the value of the $bias$ means how desirable you want to make that decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following perceptron has two inputs, each with weight -2, and an overall bias of 3 like:\n",
    "![perceptron](../picture/chapter1/p4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we see that input 00 produces output 1, since (-2)x0 + (-2)x0 + 3 = 3 is positive. Similar calculations show that the inputs 01 and 10 produce output 1. But the input 11 produces output 0, since (-2)x1 + (-2)x1 + 3 = -1 is negative. And so our perceptron implements a NAND gate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Becasue the NAND gate is universal for computation, that is, we can build any computation up out of NAND gate, so we can use networks of perceptrons to compute any logical function at all.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2 Sigmoid neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should know little about how neurons work in neural networks before introduce this artificial neuron. It turns out that we can devise learning algorithms which can automatically tune the weights and biases of a network of artificial neurons so that the output of network is our desired result. To see how learning might work, suppose we make a small change in some weight (or bias) in the network. What we'd like is for this small change in weight to cause only a small corresponding change in the output from the network. Here is what we want:\n",
    "![sigmoid](../picture/chapter1/p5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If it were true that a small change in a weight (or bias) causes only a small change in output, then we could use this fact to modify the weights and biases to get our network to behave more in the manner we want. The problem is that this isn't what happens when our network contains perceptrons. **In fact, a small change in the weights or bias of any single perceptron in the network can sometimes cause the output of that perceptron to completely flip, say from 0 to 1**. **That flip may then cause the behaviour of the rest of the network to completely change in some very complicated way.** So this makes perceptrons hard to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can overcome this problem by introducing a new type of artificial neuron called a $sigmoid\\;neuron$. **Sigmoid neurons are similar to perceptrons, but modified so that small changes in their weights and bias cause only a small change in their output.** That is the crucial fact which we allow a network of sigmoid neurons to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed before, sigmoid neurons are similar to perceptrons: \n",
    "\n",
    "![sigmoid](../picture/chapter1/p6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sigmoid neuron has inputs, $x_1$, $x_2$,....**But instead of being just 0 or 1, these inputs can also take on any values between 0 and 1**. Also just like a perceptron, the sigmoid neuron has weights for each input, $w_1$, $w_2$,..., and an overall bias, b. **But the output is not 0 or 1**. Instead, it's $\\sigma(wx + b)$, where $\\sigma$ is called sigmoid function, and is defined by:\n",
    "\n",
    "$$\\sigma(z)\\equiv \\frac{1}{1 + e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To put it all a little more explicitly, the output of a sigmoid neuron with inputs $x_1$,$x_2$,...,weights $w_1$,$w_2$,..., and bias b is\n",
    "\n",
    "$$\\frac{1}{1 + exp\\;(-\\sum_j w_jx_j - b\\;)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image of sigmoid function is as follows:\n",
    "\n",
    "![sigmoid](../picture/chapter1/p7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many similarities between perceptrons and sigmoid neurons. To understand the similarities, we define $z\\equiv w\\cdot x+b$ and conclude that \n",
    "\n",
    "$$\\lim_{z \\to +\\infty} \\sigma(z) = 1\\;and\\;\\lim_{z \\to -\\infty} \\sigma(z) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If z is a large postive number, then the output of a single perceptron is 1. And if z is very negative, then the output is 0. Perceptrons and sigmoid neurons are completely same in both respects. **Indeed, it is the smoothness of $\\sigma$ function that is crucial fact.** The smoothness of $\\sigma$ means that small changes $\\Delta w_j$ in the weights and $\\Delta b$ in the bias will produce a small change $\\Delta output$ in the ouput from the neuron. Infact, calculus tells us that $\\Delta output$ is well approximated by\n",
    "\n",
    "$$\\Delta output \\approx \\sum_{j} \\frac{\\partial \\, output}{\\partial \\, w_j} \\Delta w_j + \\frac{\\partial \\, output}{\\partial b} \\Delta b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is actually saying something very simple and exactly a good news: $\\Delta output$ is a linear function of the changes $\\Delta w_j$ and $\\Delta b$ in the weights and bias. **This linearity makes it easy to choose small changes in the weights and biases to achieve any desired small change change in the output.** One thing we should notice is that the output of a single sigmoid neuron is also a real number between 0 and 1, which is different from the output of perceptrons. However, this property is very useful when our desired classification of results are more than two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-3 The architecture of neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
